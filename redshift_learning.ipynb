{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width: 100% !important; }</style>\"))\n",
    "import psycopg2\n",
    "import boto3\n",
    "import csv\n",
    "import SomewhatSecretStuff as sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this stuff changes for file to uplaod\n",
    "localFolder = './data' #'E:/Box Sync/Vesicle Deformability/Results/Final Versions/2017_09_10 - SOPC old 1um GFP - 4 (t4-b2-a1)'\n",
    "localFile = 'cellData_2017-09-12_02-16-31'\n",
    "inDelim = ','\n",
    "inExt = 'csv'\n",
    "\n",
    "hasHeader = False\n",
    "tableName = 'prod.cell_data'\n",
    "\n",
    "#operates in place to transform one raw data row to into format suitable for COPY FROM\n",
    "experimentNo = 4\n",
    "def process_row(row):\n",
    "    row[8] = str(10.0 if row[8] == '1.#J' else min(10.0, float(row[8])))  #fix aspect ratio really large or infinite\n",
    "    row[9] = str(min(1.0, float(row[9])))  #circularity should be <= 1\n",
    "    if len(row) > 14:\n",
    "        del row[14] # remove blank column at end?\n",
    "    del row[6]  #remove Spd column\n",
    "    row.insert(0,experimentNo)  #prepend extra column for experiment #\n",
    "\n",
    "processRowFunc = process_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming file...\n",
      "uploading to S3...\n",
      "copying to Redshift...\n",
      "all good!\n"
     ]
    }
   ],
   "source": [
    "#this stuff changes if loading to different AWS stuff\n",
    "s3BucketName = 'mgrigola-redshift-upload'\n",
    "keyId = sss.AWSkeyId  #this would be poor design for real world system (is secret)\n",
    "secretKey = sss.AWSsecretKey #i think you're supposed to use environment vars or IAM something something. not this\n",
    "\n",
    "#Host is given at Redshift > Clusters > [choose cluster] > [Endpoint] (minus the port#)\n",
    "#database is created in query editor? redhsift gives you a db 'dev' and you connect to that and CREATE DATABASE somthingelse\n",
    "connData = {\n",
    "    'host':'microfluidics-data.cin96tvuwjcv.us-east-1.redshift.amazonaws.com',\n",
    "    'port':5439,\n",
    "    'dbname':'research_data',\n",
    "    'user':'mgrigola',\n",
    "    'password':sss.mgrigolaUserPass\n",
    "}\n",
    "\n",
    "#this stuff could change... mostly irrelevant config stuff\n",
    "localTempFolder = 'C:/temp'\n",
    "tempDelim = '\\t'\n",
    "tempExt = 'tsv'\n",
    "\n",
    "\n",
    "#calculate some extended/full file names from the parts that cahnge above\n",
    "localFullPath = localFolder+'/'+localFile+'.'+inExt\n",
    "localFullTempPath = localTempFolder+'/'+localFile+'.'+tempExt\n",
    "awsFileName = 'temp/'+localFile+'.'+tempExt # how S3 sees its file names (given the bucket, no base portion needed)\n",
    "awsFullFile = 's3://'+s3BucketName+'/'+awsFileName # s3://bucket/folder/file = how redshfit sees awsFileName in S3\n",
    "\n",
    "print('transforming file...')\n",
    "success = False\n",
    "\n",
    "#read original file, transform data to match aws table format, copy to local file (original unchanged, we upload the copy)\n",
    "rawColNames = {}\n",
    "with open(localFullPath,'r') as csvFile:\n",
    "    csvReader = csv.reader(csvFile, delimiter=inDelim)\n",
    "    header = next(csvReader)  #get/skip header\n",
    "    rawColNames = {colName:colNo for colNo, colName in enumerate(header) if colName != ''} #lookup dict -> header:columnNumber\n",
    "    with open(localFullTempPath,'w') as writeFile:\n",
    "        csvWriter = csv.writer(writeFile, delimiter=tempDelim, quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in csvReader:\n",
    "            if processRowFunc: processRowFunc(row)\n",
    "            csvWriter.writerow(row)\n",
    "        \n",
    "        success = True\n",
    "\n",
    "\n",
    "if not success:\n",
    "    print('failed to read/transform file - check local file name')\n",
    "else:\n",
    "    print('uploading to S3...')\n",
    "    success = False\n",
    "\n",
    "    #upload our local file to s3\n",
    "    #apparently can also use 'AWS CLI' for credentials, or IAM? or set some environment variables: AWS_ACCESS_KEY_ID / AWS_SECRET_ACCESS_KEY\n",
    "    # with boto3.Session(aws_access_key_id=keyId, aws_secret_access_key=secretKey) as session:\n",
    "    #     s3 = session.resource('s3')\n",
    "    #     s3.meta.client.upload_file(localFullTempPath, s3BucketName, awsFileName)\n",
    "    session = boto3.Session(aws_access_key_id=keyId, aws_secret_access_key=secretKey)\n",
    "    s3 = session.resource('s3')\n",
    "    s3.meta.client.upload_file(localFullTempPath, s3BucketName, awsFileName)\n",
    "    success = True\n",
    "\n",
    "\n",
    "if not success:\n",
    "    print('failed to upload to S3? Check if file exists in S3. Maybe access key is wrong?')\n",
    "else:\n",
    "    print('copying to Redshift...')\n",
    "    success = False\n",
    "    \n",
    "    headerRows = 0 if not hasHeader else int(hasHeader)\n",
    "    \n",
    "    #open connection to redshift cluster + database.\n",
    "    with psycopg2.connect(**connData) as conn:\n",
    "        #conn = psycopg2.connect(**connData)  #this snazzy syntax, same as passing each item in the dict like key='value'\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        #the SQL query that runs on Redshift. Care with the quotes. This COPY FROM will append the new data (not overwrite). Duplicate/existing key will yield key error and failure\n",
    "        copyStatement = \"\\\n",
    "        COPY {} FROM '{}' DELIMITER '{}' \\\n",
    "        CREDENTIALS 'aws_access_key_id={};aws_secret_access_key={}' \\\n",
    "        IGNOREHEADER {}  TRUNCATECOLUMNS  ROUNDEC  NULL AS '1.#J' \\\n",
    "        ;\".format(tableName, awsFullFile, tempDelim, keyId, secretKey, headerRows)  #can also commit here:  <query>; commit;\n",
    "\n",
    "        cur.execute(copyStatement)\n",
    "        #print('ok?', conn.status) #psycopg2.extensions.STATUS_READY=1 #psycopg2.extensions.STATUS_IN_TRANSACTION=2 # i don't get the status...\n",
    "        conn.commit()  #need the commit here! not autocommitted like thru editor\n",
    "        #conn.close()  #don't need this if doing the with thing\n",
    "        success = True\n",
    "\n",
    "if not success:\n",
    "    print('something went wrong...')\n",
    "else:\n",
    "    print('all good!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "localFileName = './data' #'C:/Users/m/Desktop/AWS/test_copy.csv'\n",
    "\n",
    "targetFileName = 'test_copy.csv'\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=keyId, aws_secret_access_key=secretKey)\n",
    "#client = boto3.client('s3', aws_access_key_id=keyId, aws_secret_access_key=secretKey)\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "s3.meta.client.upload_file(localFileName, bucketName, targetFileName)\n",
    "\n",
    "#bucket = s3.Bucket('mgrigola-redshift-upload')\n",
    "#myFile = s3.Object(bucket_name='mgrigola-redshift-upload', key='test/cellData_2017-09-12_01-04-24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inFile = 'E:/Box Sync/Vesicle Deformability/Results/Final Versions/2017_09_10 - SOPC old 1um GFP - 2 (t4-b2-a1)/cellData_2017-09-12_01-50-00.csv'\n",
    "outFile = 'C:/temp/cell_data.tsv'\n",
    "inDelim = ','\n",
    "outDelim = '\\t'\n",
    "experimentNo = 1\n",
    "\n",
    "#operates in place\n",
    "def process_row(row, expNo):\n",
    "    row[8] = str(10.0 if row[8] == '1.#J' else min(10.0, float(row[8])))  #fix aspect ratio really large or infinite\n",
    "    row[9] = str(min(1.0, float(row[9])))  #circularity should be <= 1\n",
    "    if len(row) > 14:\n",
    "        del row[14] # remove blank column at end?\n",
    "    del row[6]  #remove Spd column\n",
    "    row.insert(0,expNo)  #prepend extra column for experiment #\n",
    "\n",
    "\n",
    "colNames = {}\n",
    "with open(inFile,'r') as csvFile:\n",
    "    csvReader = csv.reader(csvFile, delimiter=inDelim)\n",
    "    header = next(csvReader)  #get/skip header\n",
    "    colNames = {colName:colNo for colNo, colName in enumerate(header) if colName != ''} #lookup dict -> header:columnNumber\n",
    "    with open(outFile,'w') as writeFile:\n",
    "        csvWriter = csv.writer(writeFile, delimiter=outDelim, quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in csvReader:\n",
    "            process_row(row, experimentNo)\n",
    "            csvWriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fileName = 'E:/Box Sync/Vesicle Deformability/Results/Final Versions/2017_09_10 - SOPC old 1um GFP - 1 (t4-b2-a1)/cellData_2017-09-12_01-04-24.csv'\n",
    "\n",
    "df = pd.read_csv(fileName, sep=',', header=[0], na_values=['1.#J'])\n",
    "if 'Unnamed: 14' in df.columns:\n",
    "    df = df.drop('Unnamed: 14',1)\n",
    "if 'Spd' in df.columns:\n",
    "    df = df.drop('Unnamed: 14',1)\n",
    "df['Aspc'] = df['Aspc'].apply(lambda x: min(x,10.0))\n",
    "df['Circ'] = df['Circ'].apply(lambda x: min(x,1.0), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alternative connection syntax\n",
    "cHost = 'microfluidics-data.cin96tvuwjcv.us-east-1.redshift.amazonaws.com'\n",
    "cUser = 'mgrigola'\n",
    "cPort = 5439\n",
    "cPass = sss.mgrigolaUserPass\n",
    "cDbName = 'research_data'\n",
    "conn = psycopg2.connect(host=cHost, user=cUser, port=cPort, password=cPass, dbname=cDbName)\n",
    "\n",
    "#globalFileName = 'https://s3.amazonaws.com/mgrigola-redshift-upload/2017_09_10+-+SOPC+old+1um+GFP+-+1+(t4-b2-a1)/cellData_2017-09-12_01-04-24.csv'\n",
    "#fileName = 's3://mgrigola-redshift-upload/2017_09_10 - SOPC old 1um GFP - 1 (t4-b2-a1)/cellData_2017-09-12_01-04-24.csv'\n",
    "#awsFileName = 's3://mgrigola-redshift-upload/test/cellData_2017-09-12_01-04-24.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
